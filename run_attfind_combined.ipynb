{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T10:38:21.605717Z",
     "iopub.status.busy": "2022-02-01T10:38:21.604951Z",
     "iopub.status.idle": "2022-02-01T10:38:23.508565Z",
     "shell.execute_reply": "2022-02-01T10:38:23.507609Z",
     "shell.execute_reply.started": "2022-02-01T10:38:21.605659Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "import multiprocessing\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import ast\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import requests\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from shutil import copyfile\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mobilenet_classifier import MobileNet\n",
    "from resnet_classifier import ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Important! Choose the model and classifier you want to use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_names = {\"plant\", \"faces_old\", \"faces_new\", \"ProstateCa\"}\n",
    "model_to_use = \"faces_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T10:38:23.553183Z",
     "iopub.status.busy": "2022-02-01T10:38:23.550977Z",
     "iopub.status.idle": "2022-02-01T10:38:34.787763Z",
     "shell.execute_reply": "2022-02-01T10:38:34.783749Z",
     "shell.execute_reply.started": "2022-02-01T10:38:23.553145Z"
    }
   },
   "outputs": [],
   "source": [
    "stylex_path = None\n",
    "classifier_name = None\n",
    "data = None\n",
    "USE_OLD_ARCHITECTURE = False\n",
    "\n",
    "if model_to_use == \"plant\":\n",
    "    stylex_path = \"models/old_plant_mobilenet/model_260.pt\"\n",
    "    data = \"../data/plant_village/all\" # Plant dataset\n",
    "    classifier_name = \"mobilenet-64px-plant.pt\" # Only use mobilenet for plants\n",
    "    USE_OLD_ARCHITECTURE = True\n",
    "\n",
    "elif model_to_use == \"faces_old\":\n",
    "    stylex_path = \"models/old_faces_gender_mobilenet/model_134.pt\"\n",
    "    data = \"../data/Kaggle_FFHQ_Resized_256px/flickrfaceshq-dataset-nvidia-resized-256px/resized\" # FFHQ faces dataset\n",
    "    classifier_name = \"resnet-18-64px-gender.pt\"  # Use ResNet for all the gender related ones, even the one trained on mobilenet\n",
    "    USE_OLD_ARCHITECTURE = True\n",
    "\n",
    "elif model_to_use == \"faces_new\":\n",
    "    stylex_path = \"/code/ProstateCa/ProjectedEx/models/FFHQ/model_184.pt\"\n",
    "    data = \"/code/ProstateCa/ProjectedEx/data/Kaggle_FFHQ_Resized_256px/flickrfaceshq-dataset-nvidia-resized-256px/resized\" # FFHQ faces dataset\n",
    "    classifier_name = \"/code/ProstateCa/asset/v1/resnet-18-64px-gender-classifier.pt\"  # Use ResNet for all the gender related ones, even the one trained on mobilenet\n",
    "    USE_OLD_ARCHITECTURE = True\n",
    "elif model_to_use == \"ProstateCa\":\n",
    "    stylex_path = \"/code/ProstateCa/model_32.pt\"\n",
    "    data = \"/code/ProstateCa/combinepng\" # FFHQ faces dataset\n",
    "    classifier_name = \"/code/ProstateCa/ProstateCa/stylex/saved_models/combine_mobilenet.pth\"  # Use ResNet for all the gender related ones, even the one trained on mobilenet\n",
    "    USE_OLD_ARCHITECTURE = True\n",
    "\n",
    "\n",
    "if USE_OLD_ARCHITECTURE:\n",
    "    from stylex_train import StylEx, Dataset, DistributedSampler, MNIST_1vA\n",
    "    from stylex_train import image_noise, styles_def_to_tensor, make_weights_for_balanced_classes, cycle, default\n",
    "else:\n",
    "    from stylex_train_new import StylEx, Dataset, DistributedSampler, MNIST_1vA\n",
    "    from stylex_train_new import image_noise, styles_def_to_tensor, make_weights_for_balanced_classes, cycle, default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_discriminator_threshold(stylex,\n",
    "                                 classifier,\n",
    "                                 dataloader,\n",
    "                                 num_images,\n",
    "                                 threshold_folder,\n",
    "                                 dataset_name,\n",
    "                                 image_size,\n",
    "                                 batch_size,\n",
    "                                 cuda_rank):\n",
    "\n",
    "    d = h5py.File(os.path.join(threshold_folder, 'discriminator_threshold.hdf5'), 'w')\n",
    "    discriminator_results = d.create_dataset(\"discriminator_outputs\", (num_images, 1), dtype='f')\n",
    "    generated_images = d.create_dataset(\"generated_images\", (num_images, 3, image_size, image_size), dtype='f')\n",
    "    \n",
    "    print(\"Extracting discriminator results\")\n",
    "    for image_index in tqdm.tqdm(range(num_images)):\n",
    "        image = next(dataloader)\n",
    "        image = image.cuda(cuda_rank)\n",
    "            \n",
    "        image_logits = classifier.classify_images(image)\n",
    "        w = stylex.encoder(image).unsqueeze(0)\n",
    "\n",
    "        latent_w = None\n",
    "\n",
    "        if USE_OLD_ARCHITECTURE:\n",
    "            latent_w = [(torch.cat((w, image_logits), dim=1), stylex.G.num_layers)]\n",
    "        else:\n",
    "            latent_w = [(torch.cat((w, F.softmax(image_logits, dim=1)), dim=1), stylex.G.num_layers)]\n",
    "\n",
    "        w_latent_tensor = styles_def_to_tensor(latent_w)\n",
    "        image_generated, style_coords = stylex.G(w_latent_tensor, noise, get_style_coords=True)\n",
    "\n",
    "        discriminator_output = None\n",
    "\n",
    "        if USE_OLD_ARCHITECTURE:\n",
    "            discriminator_output = discriminator_filter(stylex.D, image_generated, None)\n",
    "        else:\n",
    "            discriminator_output = discriminator_filter(stylex.D, image_generated, None, probabilities=F.softmax(classifier.classify_images(image_generated), dim=1))\n",
    "\n",
    "        discriminator_results[image_index] = discriminator_output.cpu()\n",
    "        generated_images[image_index] = image_generated.cpu()\n",
    "        \n",
    "    d.close()\n",
    "\n",
    "# TODO: Upscaling\n",
    "def plot_image(tensor) -> None:\n",
    "    grid = make_grid(tensor,nrow=5)\n",
    "    ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    display(im)\n",
    "    \n",
    "    \n",
    "def load_hdf5_results(data_file, name, threshold):\n",
    "    return np.array(data_file[name])[0:threshold]\n",
    "\n",
    "\n",
    "def set_data_src(folder='./', dataset_name=None, image_size=64, batch_size=1, num_workers=4, is_ddp=False, rank=0, world_size=1):\n",
    "    if dataset_name is None:\n",
    "        dataset = Dataset(folder, image_size, transparent=False)\n",
    "        num_workers = default(num_workers, NUM_CORES if not is_ddp else 0)\n",
    "\n",
    "        sampler = DistributedSampler(dataset, rank=rank, num_replicas=world_size,\n",
    "                                     shuffle=True) if is_ddp else None\n",
    "\n",
    "        dataloader = DataLoader(dataset, num_workers=num_workers,\n",
    "                                     batch_size=math.ceil(batch_size / world_size), sampler=sampler,\n",
    "                                     shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "    elif dataset_name == 'MNIST':\n",
    "        dataset = MNIST_1vA(digit=8)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"This dataset is not supported yet. Please use dataset_name = None.\")\n",
    "\n",
    "    loader = cycle(dataloader)\n",
    "    return dataset, loader\n",
    "\n",
    "\n",
    "def sindex_to_block_idx_and_index(generator, sindex):\n",
    "    tmp_idx = sindex\n",
    "\n",
    "    block_idx = None\n",
    "    idx = None\n",
    "\n",
    "    for idx, block in enumerate(generator.blocks):\n",
    "        if tmp_idx < block.num_style_coords:\n",
    "            block_idx = idx\n",
    "            idx = tmp_idx\n",
    "            break\n",
    "        else:\n",
    "            tmp_idx = tmp_idx - block.num_style_coords\n",
    "\n",
    "    return block_idx, idx\n",
    "\n",
    "\n",
    "def get_min_max_style_vectors(style_coordinates):\n",
    "    minimums = None\n",
    "    maximums = None\n",
    "\n",
    "    for style_coords in style_coordinates:\n",
    "        if minimums is None or maximums is None:\n",
    "            minimums = style_coords\n",
    "            maximums = style_coords\n",
    "        else:\n",
    "            minimums = torch.minimum(minimums, style_coords)\n",
    "            maximums = torch.maximum(maximums, style_coords)\n",
    "    \n",
    "    if minimums == None:\n",
    "        raise ValueError('No images pass the threshold check')\n",
    "\n",
    "    return minimums, maximums\n",
    "\n",
    "\n",
    "def discriminator_filter(discriminator, generated_image, threshold, probabilities=None):\n",
    "    if probabilities is not None:\n",
    "        output_generated = discriminator(generated_image, probabilities=probabilities)\n",
    "    else:\n",
    "        output_generated = discriminator(generated_image)\n",
    "    \n",
    "    if threshold == None:\n",
    "        return output_generated\n",
    "    if output_generated < threshold:\n",
    "        return (False, output_generated)\n",
    "    else:\n",
    "        return (True, output_generated)\n",
    "    \n",
    "\n",
    "def attfind_extraction(dataloader,\n",
    "                       num_images,\n",
    "                       results_folder,\n",
    "                       stylex,\n",
    "                       classifier,\n",
    "                       dataset_name,\n",
    "                       noise,\n",
    "                       num_style_coords,\n",
    "                       shift_size,\n",
    "                       discriminator_threshold,\n",
    "                       image_size=64,\n",
    "                       batch_size=1,\n",
    "                       cuda_rank = 0,\n",
    "                       use_discriminator=False):\n",
    "    \n",
    "    if batch_size != 1:\n",
    "        raise ValueError('Please use a batch_size equal to 1')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        style_change_effects = torch.zeros((num_images, 2, num_style_coords, 2)).cuda(cuda_rank)\n",
    "        image_latents = torch.zeros((num_images, 514)).cuda(cuda_rank)\n",
    "        generated_image_classifications = torch.zeros((num_images, 2)).cuda(cuda_rank)\n",
    "        style_coordinates = torch.zeros((num_images, num_style_coords)).cuda(cuda_rank)\n",
    "        original_images = torch.zeros((num_images, 3, image_size, image_size)).cuda(cuda_rank)\n",
    "        discriminator_results = torch.zeros((num_images, 1)).cuda(cuda_rank)\n",
    "        \n",
    "        # Select the images we want to use if use_discriminator is activated\n",
    "        images_found = 0\n",
    "\n",
    "        dataloader = iter(dataloader)\n",
    "\n",
    "        for batch in dataloader:\n",
    "            print(f\"\\rFound {images_found}/{num_images} images\".format(images_found), end=\"\")\n",
    "            if images_found >= num_images:\n",
    "                break\n",
    "\n",
    "            batch = batch.cuda(cuda_rank)\n",
    "            encoder_output = stylex.encoder(batch).unsqueeze(0)\n",
    "            real_classified_logits = classifier.classify_images(batch)\n",
    "\n",
    "            concat_w_tensor = None\n",
    "\n",
    "            if USE_OLD_ARCHITECTURE:\n",
    "                concat_w_tensor = torch.cat((encoder_output, real_classified_logits), dim=1)\n",
    "            else:\n",
    "                concat_w_tensor = torch.cat((encoder_output, F.softmax(real_classified_logits, dim=1)), dim=1)\n",
    "\n",
    "            latent_w = [(concat_w_tensor, stylex.G.num_layers)]\n",
    "            w_latent_tensor = styles_def_to_tensor(latent_w)\n",
    "            generated_image, style_coords = stylex.G(w_latent_tensor, noise, get_style_coords=True)\n",
    "\n",
    "            skip, discriminator_output = None, None\n",
    "\n",
    "            if USE_OLD_ARCHITECTURE:\n",
    "                skip, discriminator_output = discriminator_filter(stylex.D, generated_image, discriminator_threshold)\n",
    "            else:\n",
    "                skip, discriminator_output = discriminator_filter(stylex.D, generated_image, discriminator_threshold, probabilities=F.softmax(classifier.classify_images(generated_image), dim=1))\n",
    "            \n",
    "            if use_discriminator and skip:\n",
    "                continue\n",
    "            else:\n",
    "                original_images[images_found] = batch\n",
    "                image_latents[images_found] = concat_w_tensor\n",
    "                style_coordinates[images_found] = style_coords\n",
    "                discriminator_results[images_found] = discriminator_output\n",
    "                generated_image_classifications[images_found] = classifier.classify_images(generated_image)\n",
    "\n",
    "                images_found += 1\n",
    "\n",
    "        print()\n",
    "        print(\"Retrieving min and max\")\n",
    "        minima, maxima = get_min_max_style_vectors(style_coordinates)\n",
    "        \n",
    "        print(\"Exploring StyleSpace attributes\")\n",
    "        \n",
    "        #for filtered_image in filtered_images:  #TODO: Change this to a for loop\n",
    "        image_index = 0\n",
    "        for image_index in range(images_found):\n",
    "            style_coords = style_coordinates[image_index]\n",
    "            image_generated_logits = generated_image_classifications[image_index]\n",
    "\n",
    "            concat_w_tensor = image_latents[image_index].unsqueeze(0)\n",
    "            latent_w = [(concat_w_tensor, stylex.G.num_layers)]\n",
    "            w_latent_tensor = styles_def_to_tensor(latent_w)\n",
    "\n",
    "            style_change_effect = torch.Tensor(1, 2, num_style_coords, 2).cuda(cuda_rank)\n",
    "\n",
    "            for sindex in tqdm.tqdm(range(num_style_coords)):\n",
    "\n",
    "                block_idx, weight_idx = sindex_to_block_idx_and_index(stylex.G, sindex)\n",
    "                block = stylex.G.blocks[block_idx]\n",
    "\n",
    "                current_style_layer = None\n",
    "                one_hot = None\n",
    "\n",
    "                if weight_idx < block.input_channels:\n",
    "                    current_style_layer = block.to_style1\n",
    "                    one_hot = torch.zeros((1, block.input_channels)).cuda(cuda_rank)\n",
    "                else:\n",
    "                    weight_idx -= block.input_channels\n",
    "                    current_style_layer = block.to_style2\n",
    "                    one_hot = torch.zeros((1, block.filters)).cuda(cuda_rank)\n",
    "\n",
    "                one_hot[:, weight_idx] = 1\n",
    "\n",
    "                s_shift_down = one_hot * ((minima[sindex] - style_coords[sindex]) * shift_size)\n",
    "                s_shift_up = one_hot * ((maxima[sindex] - style_coords[sindex]) * shift_size)\n",
    "\n",
    "                for direction_index, shift in enumerate([s_shift_down, s_shift_up]):\n",
    "\n",
    "                    shift = shift.squeeze(0)\n",
    "\n",
    "                    current_style_layer.bias += shift\n",
    "                    perturbed_generated_images = stylex.G(w_latent_tensor, noise)\n",
    "\n",
    "                    shift_logits = classifier.classify_images(perturbed_generated_images)\n",
    "                    style_change_effect[0, direction_index, sindex] = shift_logits - image_generated_logits\n",
    "            \n",
    "                    current_style_layer.bias -= shift\n",
    "\n",
    "            style_change_effects[image_index] = style_change_effect\n",
    "            image_index += 1\n",
    "\n",
    "        \n",
    "        print(\"Initializing file\")\n",
    "        f = h5py.File(os.path.join(results_folder, 'style_change_records.hdf5'), 'w')\n",
    "        file_style_change_effects = f.create_dataset('style_change', (num_images, 2, num_style_coords, 2), dtype='f')\n",
    "        file_image_latents = f.create_dataset('latents', (num_images, 514), dtype='f')\n",
    "        file_generated_image_classifications = f.create_dataset('base_prob', (num_images, 2), dtype='f')\n",
    "        file_save_minima = f.create_dataset(\"minima\", (1, num_style_coords), dtype='f')\n",
    "        file_save_maxima = f.create_dataset(\"maxima\", (1, num_style_coords), dtype='f')\n",
    "        file_style_coordinates = f.create_dataset(\"style_coordinates\", (num_images, num_style_coords), dtype='f')\n",
    "        file_original_images = f.create_dataset(\"original_images\", (num_images, 3, image_size, image_size), dtype='f')\n",
    "        file_save_noise = f.create_dataset(\"noise\", (1, image_size, image_size, 1), dtype='f')\n",
    "        file_discriminator_results = f.create_dataset(\"discriminator\", (num_images, 1), dtype='f')\n",
    "\n",
    "        file_style_change_effects[:] = style_change_effects.cpu()\n",
    "        file_image_latents[:] = image_latents.cpu()\n",
    "        file_generated_image_classifications[:] = generated_image_classifications.cpu()\n",
    "        file_style_coordinates[:] = style_coordinates.cpu()\n",
    "        file_original_images[:] = original_images.cpu()\n",
    "        file_discriminator_results[:] = discriminator_results.cpu()\n",
    "        \n",
    "        \n",
    "        file_save_noise[0] = noise.cpu()\n",
    "        file_save_minima[0] = minima.cpu()\n",
    "        file_save_maxima[0] = maxima.cpu()\n",
    "                \n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T10:38:39.319654Z",
     "iopub.status.busy": "2022-02-01T10:38:39.319364Z",
     "iopub.status.idle": "2022-02-01T10:38:39.325286Z",
     "shell.execute_reply": "2022-02-01T10:38:39.324211Z",
     "shell.execute_reply.started": "2022-02-01T10:38:39.319625Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_loader(stylex_path,\n",
    "                   classifier_name,\n",
    "                   image_size,\n",
    "                   cuda_rank):\n",
    "\n",
    "    init_stylex = StylEx(image_size=image_size, transparent=False)\n",
    "    init_stylex.load_state_dict(torch.load(stylex_path)[\"StylEx\"])\n",
    "\n",
    "    init_classifier = None\n",
    "\n",
    "    if \"mobilenet\" in classifier_name.lower():\n",
    "        init_classifier = MobileNet(classifier_name, cuda_rank=cuda_rank, output_size=2, image_size=image_size)\n",
    "    elif \"resnet\" in classifier_name.lower():\n",
    "        init_classifier = ResNet(classifier_name, cuda_rank=cuda_rank, output_size=2, image_size=image_size)\n",
    "    else:\n",
    "        raise NotImplementedError(\"This classifier is not supported yet, please add support or change the filename to contain MobileNet or ResNet.\")\n",
    "    return init_stylex, init_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T10:39:01.850513Z",
     "iopub.status.busy": "2022-02-01T10:39:01.849719Z",
     "iopub.status.idle": "2022-02-01T10:39:01.858374Z",
     "shell.execute_reply": "2022-02-01T10:39:01.856761Z",
     "shell.execute_reply.started": "2022-02-01T10:39:01.850462Z"
    }
   },
   "outputs": [],
   "source": [
    "results_folder = './'\n",
    "threshold_folder = './'\n",
    "dataset_name = None         # for any dataset that is not MNIST\n",
    "\n",
    "num_style_coords = 2464     # TODO: Still hard coded since I didnt know how to retrieve this from the model\n",
    "image_size = 64             # Depends on dataset\n",
    "cuda_rank = 0\n",
    "num_images = 10            # Will take approximately 5 hours for 250 image of size 64x64\n",
    "shift_size = 1            # Magnitude of the step taken in the stylespace\n",
    "batch_size = 1              # Code currently only supports a batch_size of 1\n",
    "noise = image_noise(batch_size, image_size, device=cuda_rank)\n",
    "use_discriminator = False\n",
    "discriminator_threshold = -0.5 # specific for plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T00:57:09.658632Z",
     "iopub.status.busy": "2022-02-01T00:57:09.658388Z",
     "iopub.status.idle": "2022-02-01T00:57:09.700492Z",
     "shell.execute_reply": "2022-02-01T00:57:09.699508Z",
     "shell.execute_reply.started": "2022-02-01T00:57:09.658604Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset, dataloader = set_data_src(folder=data, dataset_name=dataset_name, image_size=image_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T10:39:18.467238Z",
     "iopub.status.busy": "2022-02-01T10:39:18.46651Z",
     "iopub.status.idle": "2022-02-01T10:39:24.803742Z",
     "shell.execute_reply": "2022-02-01T10:39:24.802945Z",
     "shell.execute_reply.started": "2022-02-01T10:39:18.467199Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "stylex, classifier = model_loader(stylex_path = stylex_path,\n",
    "                                  classifier_name = classifier_name,\n",
    "                                  image_size = image_size,\n",
    "                                  cuda_rank = cuda_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T00:57:17.170137Z",
     "iopub.status.busy": "2022-02-01T00:57:17.169828Z",
     "iopub.status.idle": "2022-02-01T04:09:15.87518Z",
     "shell.execute_reply": "2022-02-01T04:09:15.873929Z",
     "shell.execute_reply.started": "2022-02-01T00:57:17.170096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10/10 images\n",
      "Retrieving min and max\n",
      "Exploring StyleSpace attributes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2464/2464 [00:27<00:00, 88.08it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 89.88it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 90.68it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 89.18it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 89.35it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 90.35it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 90.29it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 89.33it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 89.47it/s]\n",
      "100%|██████████| 2464/2464 [00:27<00:00, 89.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Careful! Running this will overwrite the saved stylespace exploration file.\n",
    "try:\n",
    "    os.remove(\"./style_change_records.hdf5\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "attfind_extraction(dataloader = dataloader,\n",
    "                   num_images = num_images,\n",
    "                   results_folder = results_folder,\n",
    "                   dataset_name = dataset_name,\n",
    "                   stylex = stylex,\n",
    "                   classifier = classifier,\n",
    "                   noise=noise,\n",
    "                   num_style_coords = num_style_coords,\n",
    "                   shift_size = shift_size,\n",
    "                   batch_size = batch_size,\n",
    "                   use_discriminator = use_discriminator,\n",
    "                   discriminator_threshold = discriminator_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T10:39:31.697493Z",
     "iopub.status.busy": "2022-02-01T10:39:31.697101Z",
     "iopub.status.idle": "2022-02-01T10:39:31.702264Z",
     "shell.execute_reply": "2022-02-01T10:39:31.701597Z",
     "shell.execute_reply.started": "2022-02-01T10:39:31.697456Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_unstable_images(style_change_effect,\n",
    "                           effect_threshold = 0.3,\n",
    "                           num_indices_threshold = 150):\n",
    "\n",
    "    unstable_images = (np.sum(np.abs(style_change_effect) > effect_threshold, axis=(1, 2, 3)) > num_indices_threshold)\n",
    "    style_change_effect[unstable_images] = 0\n",
    "    \n",
    "    return style_change_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:01:07.378238Z",
     "iopub.status.busy": "2022-02-01T11:01:07.377922Z",
     "iopub.status.idle": "2022-02-01T11:01:07.556533Z",
     "shell.execute_reply": "2022-02-01T11:01:07.555559Z",
     "shell.execute_reply.started": "2022-02-01T11:01:07.378204Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold_index = 501\n",
    "\n",
    "hf = h5py.File('./style_change_records.hdf5', 'r')\n",
    "\n",
    "style_change_effect = load_hdf5_results(hf, \"style_change\", threshold_index)\n",
    "W_values = load_hdf5_results(hf, \"latents\", threshold_index)\n",
    "base_probs = load_hdf5_results(hf, \"base_prob\", threshold_index)\n",
    "all_style_vectors = load_hdf5_results(hf, \"style_coordinates\", threshold_index)\n",
    "original_images = load_hdf5_results(hf, \"original_images\", threshold_index)\n",
    "discriminator_results = load_hdf5_results(hf, \"discriminator\", threshold_index)\n",
    "\n",
    "saved_noise = np.array(hf[\"noise\"])\n",
    "style_min = np.squeeze(np.array(hf[\"minima\"]))\n",
    "style_max = np.squeeze(np.array(hf[\"maxima\"]))\n",
    "\n",
    "#style_change_effect = filter_unstable_images(style_change_effect)\n",
    "saved_noise = torch.Tensor(saved_noise).cuda(cuda_rank)\n",
    "\n",
    "all_style_vectors_distances = np.zeros((all_style_vectors.shape[0], all_style_vectors.shape[1], 2))\n",
    "all_style_vectors_distances[:,:, 0] = all_style_vectors - np.tile(style_min, (all_style_vectors.shape[0], 1))\n",
    "all_style_vectors_distances[:,:, 1] = np.tile(style_max, (all_style_vectors.shape[0], 1)) - all_style_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If the next code block gives an \"arrays used as indices must be of integer (or boolean) type\" you might want to run with more images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:01:10.774324Z",
     "iopub.status.busy": "2022-02-01T11:01:10.773712Z",
     "iopub.status.idle": "2022-02-01T11:01:10.788958Z",
     "shell.execute_reply": "2022-02-01T11:01:10.787865Z",
     "shell.execute_reply.started": "2022-02-01T11:01:10.774283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0, 1 images.\n",
      "Class 1, 9 images.\n"
     ]
    }
   ],
   "source": [
    "all_labels = np.argmax(base_probs, axis=1)\n",
    "style_effect_classes = {}\n",
    "W_classes = {}\n",
    "style_vectors_distances_classes = {}\n",
    "all_style_vectors_classes = {}\n",
    "\n",
    "for img_ind in range(2):\n",
    "    img_inx = np.array([i for i in range(all_labels.shape[0]) if all_labels[i] == img_ind])\n",
    "    curr_style_effect = np.zeros((len(img_inx), style_change_effect.shape[1], style_change_effect.shape[2], style_change_effect.shape[3]))\n",
    "    curr_w = np.zeros((len(img_inx), W_values.shape[1]))\n",
    "    curr_style_vector_distances = np.zeros((len(img_inx), style_change_effect.shape[2], 2))\n",
    "    for k, i in enumerate(img_inx):\n",
    "        curr_style_effect[k, :, :] = style_change_effect[i, :, :, :]\n",
    "        curr_w[k, :] = W_values[i, :]\n",
    "        curr_style_vector_distances[k, :, :] = all_style_vectors_distances[i, :, :]\n",
    "    style_effect_classes[img_ind] = curr_style_effect\n",
    "    W_classes[img_ind] = curr_w\n",
    "    style_vectors_distances_classes[img_ind] = curr_style_vector_distances\n",
    "    all_style_vectors_classes[img_ind] = all_style_vectors[img_inx]\n",
    "    print(f'Class {img_ind}, {len(img_inx)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:01:14.288183Z",
     "iopub.status.busy": "2022-02-01T11:01:14.28743Z",
     "iopub.status.idle": "2022-02-01T11:01:14.296321Z",
     "shell.execute_reply": "2022-02-01T11:01:14.295586Z",
     "shell.execute_reply.started": "2022-02-01T11:01:14.288133Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_significant_styles(style_change_effect,\n",
    "                            num_indices,\n",
    "                            class_index,\n",
    "                            generator,\n",
    "                            classifier,\n",
    "                            all_dlatents,\n",
    "                            style_min,\n",
    "                            style_max,\n",
    "                            max_image_effect = 0.2,\n",
    "                            label_size = 2,\n",
    "                            sindex_offset = 0):\n",
    "  \n",
    "    num_images = style_change_effect.shape[0]\n",
    "    #print(class_index)\n",
    "    style_effect_direction = np.maximum(0, style_change_effect[:, :, :, class_index].reshape((num_images, -1)))\n",
    "\n",
    "    images_effect = np.zeros(num_images)\n",
    "    all_sindices = []\n",
    "    discriminator_removed = []\n",
    "\n",
    "    while len(all_sindices) < num_indices:\n",
    "        next_s = np.argmax(np.mean(style_effect_direction[images_effect < max_image_effect], axis=0))\n",
    "\n",
    "        all_sindices.append(next_s)\n",
    "        images_effect += style_effect_direction[:, next_s]\n",
    "        style_effect_direction[:, next_s] = 0\n",
    "\n",
    "    return [(x // style_change_effect.shape[2], (x % style_change_effect.shape[2]) + sindex_offset) for x in all_sindices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:01:15.025175Z",
     "iopub.status.busy": "2022-02-01T11:01:15.024507Z",
     "iopub.status.idle": "2022-02-01T11:01:15.044798Z",
     "shell.execute_reply": "2022-02-01T11:01:15.04397Z",
     "shell.execute_reply.started": "2022-02-01T11:01:15.025136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directions and style indices for moving from class 1 to class 0 =  [(0, 2104), (1, 1014), (1, 1675), (1, 939)]\n",
      "Use the other direction to move for class 0 to 1.\n"
     ]
    }
   ],
   "source": [
    "label_size_clasifier = 2\n",
    "num_indices =  4\n",
    "effect_threshold = 0.5\n",
    "s_indices_and_signs_dict = {}\n",
    "\n",
    "for class_index in [0, 1]:\n",
    "    #split_ind = 1 - class_index\n",
    "    split_ind = class_index\n",
    "    all_s = style_effect_classes[split_ind]\n",
    "    all_w = W_classes[split_ind]\n",
    "\n",
    "    # Find s indicies\n",
    "    s_indices_and_signs = find_significant_styles(style_change_effect=all_s,\n",
    "                                                  num_indices=num_indices,\n",
    "                                                  class_index=class_index,\n",
    "                                                  generator=stylex.G,\n",
    "                                                  classifier=classifier,\n",
    "                                                  all_dlatents=all_w,\n",
    "                                                  style_min=style_min,\n",
    "                                                  style_max=style_max,\n",
    "                                                  max_image_effect=effect_threshold*5,\n",
    "                                                  label_size=label_size_clasifier,\n",
    "                                                  sindex_offset=0)\n",
    "\n",
    "    s_indices_and_signs_dict[class_index] = s_indices_and_signs\n",
    "\n",
    "sindex_class_0 = [sindex for _, sindex in s_indices_and_signs_dict[0]]\n",
    "all_sindex_joined_class_0 = [(1 - direction, sindex) for direction, sindex in s_indices_and_signs_dict[1] if sindex not in sindex_class_0]\n",
    "all_sindex_joined_class_0 += s_indices_and_signs_dict[0]\n",
    "scores = []\n",
    "\n",
    "for direction, sindex in all_sindex_joined_class_0:\n",
    "    other_direction = 1 if direction == 0 else 0\n",
    "    curr_score = np.mean(style_change_effect[:, direction, sindex, 0]) + np.mean(style_change_effect[:, other_direction, sindex, 1])\n",
    "    scores.append(curr_score)\n",
    "\n",
    "s_indices_and_signs = [all_sindex_joined_class_0[i] for i in np.argsort(scores)[::-1]]\n",
    "\n",
    "print('Directions and style indices for moving from class 1 to class 0 = ', s_indices_and_signs[:num_indices])\n",
    "print('Use the other direction to move for class 0 to 1.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:01:16.767841Z",
     "iopub.status.busy": "2022-02-01T11:01:16.766446Z",
     "iopub.status.idle": "2022-02-01T11:01:16.785631Z",
     "shell.execute_reply": "2022-02-01T11:01:16.785023Z",
     "shell.execute_reply.started": "2022-02-01T11:01:16.767798Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_change_image_given_dlatent(dlatent,\n",
    "                                        generator,\n",
    "                                        classifier,\n",
    "                                        class_index,\n",
    "                                        sindex,\n",
    "                                        s_style_min,\n",
    "                                        s_style_max,\n",
    "                                        style_direction_index,\n",
    "                                        shift_size,\n",
    "                                        label_size,\n",
    "                                        noise, \n",
    "                                        cuda_rank):\n",
    "\n",
    "    w_latent_tensor = styles_def_to_tensor(dlatent)\n",
    "    image_generated, style_coords = generator(w_latent_tensor, noise, get_style_coords=True)\n",
    "\n",
    "    block_idx, weight_idx = sindex_to_block_idx_and_index(generator, sindex)\n",
    "    block = generator.blocks[block_idx]\n",
    "\n",
    "    current_style_layer = None\n",
    "    one_hot = None\n",
    "\n",
    "    if weight_idx < block.input_channels:\n",
    "        current_style_layer = block.to_style1\n",
    "        one_hot = torch.zeros((1, block.input_channels)).cuda(cuda_rank)\n",
    "    else:\n",
    "        weight_idx -= block.input_channels\n",
    "        current_style_layer = block.to_style2\n",
    "        one_hot = torch.zeros((1, block.filters)).cuda(cuda_rank)\n",
    "\n",
    "    one_hot[:, weight_idx] = 1\n",
    "\n",
    "    if style_direction_index == 0:\n",
    "        shift = one_hot * ((s_style_min - style_coords[:, sindex]) * shift_size).unsqueeze(1)\n",
    "    else:\n",
    "        shift = one_hot * ((s_style_max - style_coords[:, sindex]) * shift_size).unsqueeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        shift = shift.squeeze(0)\n",
    "        current_style_layer.bias += shift\n",
    "        perturbed_generated_images, style_coords = generator(w_latent_tensor, noise, get_style_coords=True)\n",
    "        shift_logits = classifier.classify_images(perturbed_generated_images)\n",
    "        change_prob = torch.softmax(shift_logits, dim=1).cpu().detach().numpy()[0, class_index]\n",
    "        current_style_layer.bias -= shift\n",
    "\n",
    "    return perturbed_generated_images, change_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:01:17.373608Z",
     "iopub.status.busy": "2022-02-01T11:01:17.373365Z",
     "iopub.status.idle": "2022-02-01T11:01:17.379026Z",
     "shell.execute_reply": "2022-02-01T11:01:17.378137Z",
     "shell.execute_reply.started": "2022-02-01T11:01:17.37358Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_on_image(image,\n",
    "                  number,\n",
    "                  font_file,\n",
    "                  font_fill = (0, 0, 255)):\n",
    "\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image = np.clip(image, 0, 1)\n",
    "    fnt = ImageFont.truetype(font_file, 20)\n",
    "    out_image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "    draw = ImageDraw.Draw(out_image)\n",
    "    #draw.multiline_text((10, 10), ('%.3f' % number), font=fnt, fill=font_fill)\n",
    "    return np.array(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:01:17.885815Z",
     "iopub.status.busy": "2022-02-01T11:01:17.885313Z",
     "iopub.status.idle": "2022-02-01T11:01:17.898578Z",
     "shell.execute_reply": "2022-02-01T11:01:17.897737Z",
     "shell.execute_reply.started": "2022-02-01T11:01:17.885775Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_images_given_dlatent(dlatent,\n",
    "                                  generator,\n",
    "                                  classifier,\n",
    "                                  class_index,\n",
    "                                  sindex,\n",
    "                                  s_style_min,\n",
    "                                  s_style_max,\n",
    "                                  style_direction_index,\n",
    "                                  font_file,\n",
    "                                  noise,\n",
    "                                  shift_size = 2,\n",
    "                                  label_size = 2,\n",
    "                                  draw_results_on_image = True,\n",
    "                                  resolution = 64,\n",
    "                                  cuda_rank = 0,\n",
    "                                  gen_num_layers = 5):\n",
    "  \n",
    "    result_image = np.zeros((resolution, 2 * resolution, 3), np.uint8)\n",
    "    dlatent = [(torch.Tensor(dlatent).cuda(cuda_rank), gen_num_layers)]\n",
    "    w_latent_tensor = styles_def_to_tensor(dlatent)\n",
    "    base_image, style_coords = generator(w_latent_tensor, noise, get_style_coords=True)\n",
    "    result = classifier.classify_images(base_image)\n",
    "    base_prob = torch.softmax(result, dim=1).cpu().detach().numpy()[0, class_index]\n",
    "\n",
    "    if draw_results_on_image:\n",
    "        result_image[:, :resolution, :] = draw_on_image(base_image[0].cpu().detach().numpy(), base_prob, font_file)[:,:,0:3]\n",
    "    else:\n",
    "        result_image[:, :resolution, :] = (base_image[0].cpu().detach().numpy() * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "    change_image, change_prob = (generate_change_image_given_dlatent(dlatent, generator, classifier,\n",
    "                                                                     class_index, sindex,\n",
    "                                                                     s_style_min, s_style_max,\n",
    "                                                                     style_direction_index, shift_size,\n",
    "                                                                     label_size, noise=noise, cuda_rank=0))\n",
    " \n",
    "    if draw_results_on_image:\n",
    "        result_image[:, resolution:, :] = draw_on_image(change_image[0].cpu().detach().numpy(), change_prob, font_file)[:,:,0:3]\n",
    "\n",
    "    else:\n",
    "        result_image[:, resolution:, :] = (np.maxiumum(np.minimum(change_image[0].cpu().detach().numpy(), 1), -1) * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "    return (result_image, change_prob, base_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:04:33.110768Z",
     "iopub.status.busy": "2022-02-01T11:04:33.11048Z",
     "iopub.status.idle": "2022-02-01T11:04:33.12382Z",
     "shell.execute_reply": "2022-02-01T11:04:33.122732Z",
     "shell.execute_reply.started": "2022-02-01T11:04:33.110737Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_style(generator,\n",
    "                    classifier,\n",
    "                    all_dlatents,\n",
    "                    style_change_effect,\n",
    "                    style_min,\n",
    "                    style_max,\n",
    "                    sindex,\n",
    "                    style_direction_index,\n",
    "                    max_images,\n",
    "                    shift_size,\n",
    "                    font_file,\n",
    "                    noise,\n",
    "                    label_size = 2,\n",
    "                    class_index = 0,\n",
    "                    effect_threshold = 0.3,\n",
    "                    seed = None,\n",
    "                    allow_both_directions_change = False,\n",
    "                    draw_results_on_image = True):\n",
    "  \n",
    "    if allow_both_directions_change:\n",
    "        images_idx = (np.abs(style_change_effect[:, style_direction_index, sindex,\n",
    "                                             class_index]) >\n",
    "                  effect_threshold).nonzero()[0]\n",
    "    else:\n",
    "        images_idx = ((style_change_effect[:, style_direction_index, sindex,\n",
    "                                       class_index]) >\n",
    "                  effect_threshold).nonzero()[0]\n",
    "    if images_idx.size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    np.random.shuffle(images_idx)\n",
    "    images_idx = images_idx[:min(max_images*10, len(images_idx))]\n",
    "    dlatents = all_dlatents[images_idx]\n",
    "\n",
    "    result_images = []\n",
    "    for i in range(len(images_idx)):\n",
    "        cur_dlatent = dlatents[i:i + 1]\n",
    "        (result_image, base_prob, change_prob) = generate_images_given_dlatent(\n",
    "                                                 dlatent=cur_dlatent,\n",
    "                                                 generator=generator,\n",
    "                                                 classifier=classifier,\n",
    "                                                 class_index=class_index,\n",
    "                                                 sindex=sindex,\n",
    "                                                 noise=noise,\n",
    "                                                 s_style_min=style_min[sindex],\n",
    "                                                 s_style_max=style_max[sindex],\n",
    "                                                 style_direction_index=style_direction_index,\n",
    "                                                 font_file=font_file,\n",
    "                                                 shift_size=shift_size,\n",
    "                                                 label_size=label_size,\n",
    "                                                 draw_results_on_image=draw_results_on_image,\n",
    "                                                 gen_num_layers=stylex.G.num_layers)\n",
    "\n",
    "        if np.abs(change_prob - base_prob) < effect_threshold:\n",
    "            continue\n",
    "        result_images.append(result_image)\n",
    "        if len(result_images) == max_images:\n",
    "            break\n",
    "\n",
    "    if len(result_images) < 3:\n",
    "        # No point in returning results with very little images\n",
    "        return np.array([])\n",
    "    return np.concatenate(result_images[:max_images], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:04:33.294972Z",
     "iopub.status.busy": "2022-02-01T11:04:33.294496Z",
     "iopub.status.idle": "2022-02-01T11:04:33.306626Z",
     "shell.execute_reply": "2022-02-01T11:04:33.305948Z",
     "shell.execute_reply.started": "2022-02-01T11:04:33.29494Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_style_by_distance_in_s(generator,\n",
    "                                    classifier,\n",
    "                                    all_dlatents,\n",
    "                                    all_style_vectors_distances,\n",
    "                                    style_min,\n",
    "                                    style_max,\n",
    "                                    sindex,\n",
    "                                    style_sign_index,\n",
    "                                    max_images,\n",
    "                                    shift_size,\n",
    "                                    font_file,\n",
    "                                    noise,\n",
    "                                    label_size = 2,\n",
    "                                    class_index = 0,\n",
    "                                    draw_results_on_image = True,\n",
    "                                    effect_threshold = 0.1, \n",
    "                                    cuda_rank=0):\n",
    "\n",
    "    images_idx = np.argsort(all_style_vectors_distances[:, sindex, style_sign_index])[::-1]\n",
    "\n",
    "    if images_idx.size == 0:\n",
    "        print(\"images_idx size is zero\")\n",
    "        return np.array([])\n",
    "\n",
    "    images_idx = images_idx[:min(max_images*10, len(images_idx))]\n",
    "    dlatents = all_dlatents[images_idx]\n",
    "\n",
    "    result_images = []\n",
    "    for i in range(len(images_idx)):\n",
    "        cur_dlatent = dlatents[i:i + 1]\n",
    "        (result_image, change_prob, base_prob) = generate_images_given_dlatent(dlatent=cur_dlatent,\n",
    "                                                                                 generator=generator,\n",
    "                                                                                 classifier=classifier,\n",
    "                                                                                 class_index=class_index,\n",
    "                                                                                 sindex=sindex,\n",
    "                                                                                 noise=noise,\n",
    "                                                                                 s_style_min=style_min[sindex],\n",
    "                                                                                 s_style_max=style_max[sindex],\n",
    "                                                                                 style_direction_index=style_sign_index,\n",
    "                                                                                 font_file=font_file,\n",
    "                                                                                 shift_size=shift_size,\n",
    "                                                                                 label_size=label_size,\n",
    "                                                                                 draw_results_on_image=draw_results_on_image,\n",
    "                                                                                 cuda_rank = 0)\n",
    "        result_images.append(result_image)\n",
    "\n",
    "    if len(result_images) < 3:\n",
    "        return np.array([])\n",
    "    return np.concatenate(result_images[:max_images], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:04:33.455036Z",
     "iopub.status.busy": "2022-02-01T11:04:33.454843Z",
     "iopub.status.idle": "2022-02-01T11:04:33.460774Z",
     "shell.execute_reply": "2022-02-01T11:04:33.459884Z",
     "shell.execute_reply.started": "2022-02-01T11:04:33.455013Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_image(image, fmt='png'):\n",
    "\n",
    "    if image.dtype == np.float32:\n",
    "        image = np.uint8(image * 255)\n",
    "\n",
    "    if image.shape[0] == 3:\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:12:11.942699Z",
     "iopub.status.busy": "2022-02-01T11:12:11.942434Z",
     "iopub.status.idle": "2022-02-01T11:12:13.322084Z",
     "shell.execute_reply": "2022-02-01T11:12:13.320363Z",
     "shell.execute_reply.started": "2022-02-01T11:12:11.94266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no images found\n"
     ]
    }
   ],
   "source": [
    "max_images = 10\n",
    "sindex = 2449    # Put the stylespace indices from the print statement above here, together with the sign corresponding to the direction.\n",
    "class_index = 0  # The class we want to change\n",
    "#shift_sign = \"0\"\n",
    "wsign_index = 0\n",
    "label_size = 2\n",
    "\n",
    "shift_size = 2\n",
    "effect_threshold =  0.5\n",
    "split_by_class = True\n",
    "select_images_by_s_distance = True\n",
    "draw_results_on_image = True \n",
    "\n",
    "if split_by_class:\n",
    "    split_ind = 1 if class_index == 1 else 0 #1 if class_index == 0 else 0\n",
    "    all_s = style_effect_classes[split_ind]\n",
    "    all_w = W_classes[split_ind]\n",
    "    all_s_distances = style_vectors_distances_classes[split_ind]\n",
    "else:\n",
    "    all_s = style_change_effect\n",
    "    all_w = W_values\n",
    "    all_s_distances = all_style_vectors_distances\n",
    "\n",
    "font_file = './arialuni.ttf'\n",
    "if not os.path.exists(font_file):\n",
    "    r = requests.get('https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/ipwn/arialuni.ttf')\n",
    "    open(font_file, 'wb').write(r.content)\n",
    "\n",
    "if not select_images_by_s_distance:\n",
    "    yy = visualize_style(stylex.G,\n",
    "                         classifier,\n",
    "                         all_w,\n",
    "                         all_s,\n",
    "                         style_min,\n",
    "                         style_max,\n",
    "                         sindex,\n",
    "                         wsign_index,\n",
    "                         max_images=max_images,\n",
    "                         shift_size=shift_size,\n",
    "                         font_file=font_file,\n",
    "                         label_size=label_size,\n",
    "                         class_index=class_index,\n",
    "                         effect_threshold=effect_threshold,\n",
    "                         draw_results_on_image=draw_results_on_image,\n",
    "                         noise = saved_noise)\n",
    "    \n",
    "else:\n",
    "    yy = visualize_style_by_distance_in_s(stylex.G,\n",
    "                                          classifier,\n",
    "                                          all_w,\n",
    "                                          all_s_distances,\n",
    "                                          style_min,\n",
    "                                          style_max,\n",
    "                                          sindex,\n",
    "                                          wsign_index,\n",
    "                                          max_images=max_images,\n",
    "                                          shift_size=shift_size,\n",
    "                                          font_file=font_file,\n",
    "                                          label_size=label_size,\n",
    "                                          class_index=class_index,\n",
    "                                          effect_threshold=effect_threshold,\n",
    "                                          draw_results_on_image=draw_results_on_image, cuda_rank=0,\n",
    "                                          noise = saved_noise)\n",
    "\n",
    "if yy.size > 0:\n",
    "    show_image(yy)\n",
    "else:\n",
    "    print('no images found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
